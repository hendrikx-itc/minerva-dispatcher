#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Minerva Dispatcher job sweep command line script
"""
import os
import datetime
import argparse
import logging.handlers
import re
from contextlib import closing
import json
from functools import partial

from minerva.util import first
from minerva.system.job import Job
from minerva.instance import connect

from minerva_dispatcher import version, JOB_TYPE, get_job_sources

DELAY = 1800  # seconds

package_name = "minerva_dispatcher"
script_name = os.path.basename(__file__)
config_file = "{}.conf".format(script_name)


def main():
    """
    Script entry point
    """
    parser = argparse.ArgumentParser(
        description="sweep job sources for existing files/jobs"
    )

    parser.add_argument(
        "-v", "--version", action="version",
        version="%(prog)s {}".format(version.__version__)
    )

    parser.add_argument(
        "--when-queue-empty", action="store_true",
        help="only run when the job queue is empty"
    )

    parser.add_argument(
        "--verbose", action="store_true", default=False,
        help="show verbose output"
    )

    parser.add_argument(
        "--debug", action="store_true", default=False,
        help="show debug output"
    )

    args = parser.parse_args()

    logging.root.addHandler(logging.StreamHandler())

    if args.verbose:
        logging.root.setLevel(logging.INFO)

    if args.debug:
        logging.root.setLevel(logging.DEBUG)

    with closing(connect()) as conn:
        with closing(conn.cursor()) as cursor:
            job_sources = get_job_sources(cursor)

            if args.when_queue_empty:
                queue_size = get_queue_size(cursor)

                if queue_size > 0:
                    return

        conn.commit()

        for job_source_id, description in sweep(job_sources):
            process(conn, job_source_id, description)


def get_queue_size(cursor):
    query = "SELECT count(*) FROM system.job_queue"

    cursor.execute(query)

    return first(cursor.fetchone())


def sweep(job_sources):
    for job_source in job_sources:
        match_pattern = job_source.config["match_pattern"]

        regex = re.compile(match_pattern)

        uri = job_source.config["uri"]
        recursive = job_source.config["recursive"]

        logging.info("checking job source {} at {} with pattern {}".format(
            job_source.name, uri, match_pattern))

        if recursive:
            paths = (
                os.path.join(root, f)
                for root, _, files in os.walk(uri)
                for f in files if regex.match(f)
            )
        else:
            make_full_path = partial(os.path.join, uri)

            all_file_paths = map(make_full_path, os.listdir(uri))

            def file_matches(f):
                return os.path.isfile(f) and regex.match(f)

            paths = filter(file_matches, all_file_paths)

        def is_old(path):
            logging.debug(path)
            delay_delta = datetime.timedelta(seconds=DELAY)

            try:
                timestamp = os.path.getmtime(path)
            except OSError:
                return False
            else:
                modified = datetime.datetime.fromtimestamp(timestamp)

                return modified < (datetime.datetime.now() - delay_delta)

        for path in filter(is_old, paths):
            yield job_source.id, job_source.job_description(path)


def process(conn, job_source_id, job_description):
    path = job_description["uri"]

    try:
        file_size = os.path.getsize(path)
    except OSError as exc:
        logging.info("could not get size of file: {}".format(exc))
        return

    with closing(conn.cursor()) as cursor:
        Job.create(
            JOB_TYPE, json.dumps(job_description), file_size, job_source_id
        )(cursor)

    conn.commit()

    logging.info("queued {}".format(path))


if __name__ == "__main__":
    main()
