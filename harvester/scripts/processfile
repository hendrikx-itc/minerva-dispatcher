#!/usr/bin/env python
# -*- coding: utf-8 -*-
__docformat__ = "restructuredtext en"

__copyright__ = """
Copyright (C) 2008-2013 Hendrikx-ITC B.V.

Distributed under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option) any later
version.  The full license is in the file COPYING, distributed as part of
this software.
"""
import os
import sys
import logging
import json
from logging.handlers import RotatingFileHandler
import argparse
from configobj import ConfigObj
import pkg_resources
from contextlib import closing
from functools import partial
from operator import itemgetter
import re

import psycopg2.extras

from minerva.db import parse_db_url
from minerva.util import compose, expand_args, unlines, k, \
    make_tuple
from minerva.util.config import parse_size, get_defaults
from minerva.util.tabulate import render_table
from minerva.directory.helpers_v4 import name_to_datasource
from minerva.storage import get_plugin

from minerva.harvesting.fileprocessor import process_file
from minerva.harvesting.plugins import ENTRYPOINT, load_plugins


package_name = "minerva.harvester"
script_name = os.path.basename(__file__)
config_file = "{}.conf".format(script_name)


level_map = {
    "DEBUG": logging.DEBUG,
    "INFO": logging.INFO,
    "WARNING": logging.WARNING,
    "ERROR": logging.ERROR,
    "CRITICAL": logging.CRITICAL}


def main():
    configfile_path = os.path.join("/etc/minerva/", config_file)

    parser = argparse.ArgumentParser(description="Script for processing files")

    parser.add_argument("file_path", nargs="*",
        help="Path of file that will be processed")

    parser.add_argument("-p", "--plugin",
        help="Harvester plug-in to use for processing file(s)")

    parser.add_argument("-l", "--list-plugins", action="store_true",
        dest="list_plugins", help="List installed Harvester plug-ins")

    parser.add_argument("-c", "--configfile", default=configfile_path,
        dest="configfile", help="Path to config file")

    parser.add_argument("--parser-config", default="{}", dest="parserconfig",
        help="Parser specific configuration")

    parser.add_argument("--store", action="store_true", default=False,
        help="Write data to database")

    parser.add_argument("--show-progress", action="store_true",
        dest="show_progress", default=False, help="Show progressbar")

    parser.add_argument("-v", "--verbose", action="store_true", dest="verbose",
        default=False, help="Produce verbose output")

    parser.add_argument("--debug", action="store_true", dest="debug",
        default=False, help="Produce debug output")

    parser.add_argument("--filter-dn", help="filter by distinguished name")

    parser.add_argument("--filter-trend", help="filter by trend name")

    parser.add_argument("--generate-configfile",
            action=GenerateConfigFileAction, nargs=0,
            help="generate default config file")

    parser.add_argument("--datasource", dest="datasource",
        help="Datasource to use")

    parser.add_argument("--statistics", action="store_true", dest="statistics",
        default=False, help="Show statistics like number of packages, entities, etc.")

    args = parser.parse_args()

    if args.list_plugins:
        for entrypoint in pkg_resources.iter_entry_points(group=ENTRYPOINT):
            print(entrypoint.name)

        return 0

    if not os.path.isfile(args.configfile):
        print("Config file {0} could not be found.".format(args.configfile))
        return -1

    config = ConfigObj(args.configfile)

    log_level = level_map[config["log_level"]]

    if args.debug:
        log_level = logging.DEBUG

    parser_config = json.loads(args.parserconfig)

    logger = setup_logging(config["log_directory"], config["log_filename"],
        config["log_rotation_size"], log_level)

    if args.verbose:
        logger.addHandler(logging.StreamHandler())

    if not args.plugin:
        print("Harvester plug-in {0} could not be found.".format(args.plugin))
        return 1

    if args.filter_dn:
        entity_filter = re.compile(args.filter_dn).match
    else:
        entity_filter = k(True)

    if args.filter_trend:
        trend_filter = re.compile(args.filter_trend).match
    else:
        trend_filter = k(True)

    harvest_plugins = load_plugins()
    harvest_plugin = harvest_plugins.get(args.plugin, None)

    if harvest_plugin is None:
        print("Data type '{0}' not supported".format(args.plugin))
        return 1


    handle_package = partial(filter_package, entity_filter, trend_filter)
    handle_package = compose(handle_package, make_tuple)

    if args.debug:
        handle_package = compose(show_package, handle_package)

    statistics = Statistics()

    if args.statistics:
        handle_package = compose(extract_statistics(statistics), handle_package)

    if args.debug:
        db_logger = logging.getLogger("")
    else:
        db_logger = None

    db_uri = config["db_uri"]

    with closing(connect(db_uri, db_logger)) as conn:
        with closing(conn.cursor()) as cursor:
            datasource = name_to_datasource(cursor, args.datasource)

        if args.store:
            storageprovider = get_plugin(harvest_plugin.storagetype())(conn, api_version=4)

            store_raw = compose(partial(storageprovider.store_raw, datasource),
                    storageprovider.RawDataPackage)

            handle_package = compose(expand_args(store_raw), handle_package)

        for file_path in args.file_path:
            if args.verbose:
                logging.info("Processing {0}".format(file_path))

            message = (
                "Start processing file {0} using plugin {1}"
                " and config {2}").format(file_path, args.plugin, parser_config)

            logging.info(message)

            process_file(file_path, harvest_plugin, parser_config, handle_package,
                args.show_progress)

    if args.statistics:
        logging.info("{} packages extracted".format(statistics.package_count))

    return 0


class Statistics(object):
    def __init__(self):
        self.package_count = 0


def extract_statistics(stats):
    def f(package):
        stats.package_count += 1

        return package

    return f


def filter_package(entity_filter, trend_filter, package):
    trend_names, rows = package[-2:]

    filtered_trend_names = filter(trend_filter, trend_names)

    if len(filtered_trend_names) > 0:
        trend_filter_map = map(trend_filter, trend_names)

        filter_rows = compose(entity_filter, itemgetter(0))

        entity_filtered_rows = filter(filter_rows, rows)

        filtered_rows = []

        for dn, values in entity_filtered_rows:
            trend_filtered_values = [v for include, v in
                    zip(trend_filter_map, values) if include]

            trend_filtered_row = dn, trend_filtered_values

            filtered_rows.append(trend_filtered_row)
    else:
        filtered_rows = []

    return package[:-2] + (filtered_trend_names, filtered_rows)


def show_package(package):
    trend_names, rows = package[-2:]

    if rows:
        render_as_table(trend_names, rows)

    return package


def render_as_table(trend_names, rows):
    column_names = ["dn"] + list(trend_names)
    column_align = ">" * len(column_names)
    column_sizes = ["max"] * len(column_names)

    rows = [[dn] + values for dn, values in rows]

    table = render_table(column_names, column_align, column_sizes, rows)

    print(unlines(table))


class GenerateConfigFileAction(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        sys.stdout.write(get_defaults(package_name, config_file))
        sys.exit(0)


def connect(db_url, logger=None):
    scheme, user, password, host, port, database = parse_db_url(db_url)

    assert scheme == "postgresql", "Only PostgreSQL connections are supported"

    create_connection = partial(psycopg2.connect, database=database, user=user,
            password=password, host=host, port=port)

    if logger:
        conn = create_connection(connection_factory=psycopg2.extras.LoggingConnection)

        conn.initialize(logger)
    else:
        conn = create_connection()

    return conn


def setup_logging(directory, filename, rotation_size, level):
    max_log_size = parse_size(rotation_size)

    filepath = os.path.join(directory, filename)
    handler = RotatingFileHandler(filepath, maxBytes=max_log_size, backupCount=5)
    handler.setLevel(level)

    formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
    handler.setFormatter(formatter)

    root_logger = logging.getLogger("")
    root_logger.setLevel(level)
    root_logger.addHandler(handler)

    return root_logger


if __name__ == "__main__":
    sys.exit(main())
